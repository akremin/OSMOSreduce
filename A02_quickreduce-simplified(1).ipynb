{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/C/Users/kremin/Anaconda3/python.exe\n",
    "# coding: utf-8\n",
    "\n",
    "# Basic Walkthrough:\n",
    "#      1) Define everything\n",
    "#      2) Create master bias file, Save master bias file  \n",
    "#      3) Open all other files, sub master bias, save  (*c?.b.fits)\n",
    "#      4) Remove cosmics from all file types except bias  (*c?.bc.fits)\n",
    "#      5) Open flats and create master skyflat file, save\n",
    "#      6) Open all remainging types and divide out master flat, then save  (*c?.bcf.fits)\n",
    "#      7) Open all remaining types and stitch together, save  (*full.bcf.fits)\n",
    "#      8) Use fibermap files to determine aperatures\n",
    "#      9) Use aperatures to cut out same regions in thar,comp,science\n",
    "#      10) Save all 256 to files with their header tagged name in filename, along with fiber num\n",
    "#      11) Assume no curvature within tiny aperature region; fit profile to 2d spec and sum to 1d\n",
    "#      12) Fit the lines in comp spectra, save file and fit solution\n",
    "#      13) Try to fit lines in thar spectra, save file and fit solution\n",
    "#      14) Apply to science spectra\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.ndimage.filters import median_filter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from quickreduce_funcs import get_all_filedata, print_data_neatly,                               save_hdu,get_dict_temp\n",
    "\n",
    "\n",
    "# ### Define input file numbers and other required information\n",
    "# \n",
    "# Ex:\n",
    "# \n",
    "#     Bias 597-626\n",
    "#     ThAr 627,635\n",
    "#     NeHgArXe 628,629,636,637\n",
    "#     Science 631-634\n",
    "#     Fibermaps 573-577\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "biass = np.arange(597,626+1).astype(int)\n",
    "thar_lamps = np.asarray([627,635])\n",
    "comp_lamps = np.asarray([628,629,636,637])\n",
    "twiflats = np.arange(582,591+1).astype(int)\n",
    "sciences = np.arange(631,634+1).astype(int)\n",
    "fibermaps = np.arange(573,577+1).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "instrument = 'M2FS'\n",
    "mask_name = 'A02'\n",
    "config = '11C'\n",
    "# cal_lamp = ['Xe', 'Ar', 'HgNe', 'Hg', 'Ne']\n",
    "cal_lamp = ['HgAr', 'NeAr']\n",
    "# cal_lamp = ['HgAr', 'NeAr', 'Ar', 'Xe']\n",
    "# cal_lamp = ['Xenon','Argon','Neon', 'HgNe']\n",
    "# cal_lamp = ['Xe', 'Ar', 'HgNe', 'Hg', 'Ne']\n",
    "# thar_lamp = ['Th','ThAr']\n",
    "thar_lamp = ['ThAr']\n",
    "cameras = ['r']\n",
    "opamps = [1,2,3,4]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path_to_masks = os.path.abspath('../../OneDrive/Research/M2FSReductions')\n",
    "mask_subdir = mask_name\n",
    "raw_data_subdir =  'raw_data'\n",
    "filename_template = {}\n",
    "filename_template['raw'] = '{cam}{filenum:04d}c{opamp}.fits'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "make_debug_plots = False\n",
    "print_headers = True\n",
    "cut_bias_cols = True\n",
    "convert_adu_to_e = True\n",
    "load_data_from_disk_each_step = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "do_step = OrderedDict()\n",
    "do_step['stitch'] = False\n",
    "do_step['bias'] = False\n",
    "do_step['remove_crs']   = False\n",
    "do_step['ffmerge'] = False\n",
    "do_step['apcut'] = False\n",
    "do_step['wavecalib'] = True\n",
    "do_step['flat'] = False\n",
    "do_step['combine'] = False\n",
    "do_step['zfit'] = False\n",
    "\n",
    "\n",
    "# ###         Beginning of Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "date = np.datetime_as_string(np.datetime64('today', 'D'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "directory = {}\n",
    "directory['mask'] = os.path.join(path_to_masks,mask_subdir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "directory['raw_data'] =     os.path.join(directory['mask'], raw_data_subdir)\n",
    "directory['raw_stitched'] = os.path.join(directory['mask'],'raw_stitched')\n",
    "directory['product'] =      os.path.join(directory['mask'],'data_products')\n",
    "directory['twod'] =         os.path.join(directory['mask'],'twods')\n",
    "directory['oned'] =         os.path.join(directory['mask'],'oneds')\n",
    "directory['calibrated'] =   os.path.join(directory['mask'],'calibrated_oned')\n",
    "directory['summedspec'] =   os.path.join(directory['mask'],'final_oned')\n",
    "directory['zfit'] =         os.path.join(directory['mask'],'zfits')\n",
    "directory['linelists'] = os.path.join(os.curdir,'lamp_linelists','salt')\n",
    "\n",
    "for dirpath in directory.values():\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filenumbers = {'bias':biass, 'thar':thar_lamps, 'comp': comp_lamps,\n",
    "               'twiflat':twiflats, 'science': sciences, 'fibmap': fibermaps}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filename_template['base'] =      '{cam}_{imtype}_{filenum:04d}_{maskname}_'\n",
    "filename_template['stitched'] =  filename_template['base']+'stitched{tags}.fits'\n",
    "filename_template['twod'] =      filename_template['base']+'{fibername}_2d{tags}.fits'\n",
    "filename_template['oned'] =      filename_template['base']+'1d{tags}.fits'\n",
    "filename_template['combined'] =  filename_template['base']+'{fibername}_combined_1d{tags}.fits'\n",
    "filename_template['linelist'] =  '{lamp}.txt'\n",
    "\n",
    "filename_template['master_stitched'] =  '{cam}_master{imtype}_{maskname}_stitched{tags}.fits'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "setup_info  =      {    'maskname':         mask_name,\n",
    "                        'cameras':          cameras,\n",
    "                        'opamps':           opamps, \n",
    "                        'deadfibers':       None          }\n",
    "\n",
    "load_info   =       {   'datadir':          directory['raw_data'],\n",
    "                        'template':         filename_template['raw'],\n",
    "                        'tags':             ''            }\n",
    "\n",
    "save_info   =       {   'date':             date,\n",
    "                        'datadir':          directory['raw_stitched'],\n",
    "                        'template':         filename_template['stitched'],\n",
    "                        'tags':             ''             }\n",
    "\n",
    "master_info  =      {   'master_template':  filename_template['master_stitched'],\n",
    "                        'master_types':     []             }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start = 'stitch'\n",
    "for key,val in do_step.items():\n",
    "    if val:\n",
    "        start = key\n",
    "        break\n",
    "\n",
    "\n",
    "# ### How the hdudict is structured\n",
    "# \n",
    "#     dict_of_hdus  \n",
    "#          keys: science, comp, thar, flat, fibmap\n",
    "#          vals: cameras_dict\n",
    "#               \n",
    "#          cameras_dict \n",
    "#                 keys: r, b\n",
    "#                 vals: filenumbers_dict\n",
    "#                            \n",
    "#                 filenumbers_dict \n",
    "#                         keys: integer numbers\n",
    "#                         vals: opampdict\n",
    "#                                             \n",
    "#                         opampdict \n",
    "#                                keys: ints 1, 2, 3, 4\n",
    "#                                vals: hdus for each of 4 opamps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#start = 'stitch'\n",
    "if load_data_from_disk_each_step or ('stitch' == start):\n",
    "    ## Load in data\n",
    "    dict_of_hdus = get_all_filedata( filenum_dict=filenumbers, \n",
    "                                     **setup_info,**load_info,**master_info,\n",
    "                                      cut_bias_cols=cut_bias_cols, \n",
    "                                      convert_adu_to_e = convert_adu_to_e )\n",
    "    print_data_neatly(dict_of_hdus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if do_step['stitch']:\n",
    "    stitched_hdu_dict = {}\n",
    "    from quickreduce_funcs import stitch_these_camera_data\n",
    "    for imtype,camdict in dict_of_hdus.items():\n",
    "        stitched_hdu_dict[imtype] = {}\n",
    "        for camera,filenumdict in camdict.items():\n",
    "            stitched_hdu_dict[imtype][camera] = {}\n",
    "            if imtype == 'bias':\n",
    "                allfile_3darray,headers = [],[]\n",
    "            for filenum,opampdict in filenumdict.items():\n",
    "                if imtype == 'bias':\n",
    "                    outhdu = stitch_these_camera_data(opampdict,filenum,camera,imtype,mask_name,\n",
    "                                                      save_info,save_each=False)\n",
    "                    headers.append(outhdu.header)\n",
    "                    allfile_3darray.append(outhdu.data)\n",
    "                else:\n",
    "                    outhdu = stitch_these_camera_data(opampdict,filenum,camera,imtype,mask_name,\n",
    "                                                      save_info, save_each=True,\n",
    "                                                      make_plot=make_debug_plots)\n",
    "                    stitched_hdu_dict[imtype][camera][filenum] = outhdu\n",
    "                    \n",
    "            if imtype == 'bias':\n",
    "                filenum_3d_array = np.asarray(allfile_3darray)\n",
    "                filenum_median_array = np.median(filenum_3d_array,axis=0)\n",
    "                \n",
    "                header = headers[0]\n",
    "                del filenum_3d_array\n",
    "                del headers\n",
    "                \n",
    "                header.add_history(\"Median Master Bias done by quickreduce on {}\".format(date))\n",
    "\n",
    "                median_outhdu = fits.PrimaryHDU(data=filenum_median_array ,header=header)\n",
    "                outname = master_info['master_template'].format(cam=camera, imtype=imtype,maskname=mask_name, \n",
    "                                                               tags=save_info['tags'])\n",
    "                filename = os.path.join(save_info['datadir'], outname)\n",
    "                median_outhdu.writeto( filename ,overwrite=True)\n",
    "\n",
    "                if make_debug_plots:\n",
    "                    ## Plot the image\n",
    "                    plt.figure()\n",
    "                    plot_array = median_outhdu.data - np.min(median_outhdu.data) + 1e-4\n",
    "                    plt.imshow(np.log(plot_array),'gray',origin='lowerleft')\n",
    "                    plt.savefig(filename.replace('.fits','.png'),dpi=1200)\n",
    "                    plt.show()\n",
    "\n",
    "                stitched_hdu_dict[imtype][camera] = median_outhdu\n",
    "            \n",
    "    dict_of_hdus = stitched_hdu_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Now that stitching is complete. opamps = None\n",
    "if 'bias' in filenumbers.keys():\n",
    "    filenumbers.pop('bias')\n",
    "#if 'bias' not in master_info['master_types']:\n",
    "#    master_info['master_types'].append('bias')\n",
    "    \n",
    "setup_info['opamps'] = None\n",
    "load_info['datadir'] = directory['raw_stitched']\n",
    "save_info['datadir'] = directory['product']\n",
    "load_info['tags'] = ''\n",
    "save_info['tags'] = '.b'    \n",
    "save_info['template'] = filename_template['stitched']\n",
    "load_info['template'] = filename_template['stitched']\n",
    "\n",
    "if load_data_from_disk_each_step or ('bias' == start):\n",
    "        dict_of_hdus = get_all_filedata(filenum_dict=filenumbers, \n",
    "                                        **setup_info,**load_info,**master_info)\n",
    "        print_data_neatly(dict_of_hdus)\n",
    "        blank_dictofdicts = get_dict_temp(dict_of_hdus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if do_step['bias']:\n",
    "    master_bias_dict = dict_of_hdus.pop('bias')\n",
    "    debiased_hdus = blank_dictofdicts.copy()\n",
    "    for imtype,camdict in dict_of_hdus.items():\n",
    "        for camera,filenumdict in camdict.items():\n",
    "            master_bias = master_bias_dict[camera].data.astype(float)\n",
    "            for filenum in filenums:\n",
    "                filnumarray = dict_of_hdus[imtype][camera][filenum].data.astype(float)\n",
    "                header = dict_of_hdus[imtype][camera][filenum].header\n",
    "                \n",
    "                filnumarray -= master_bias\n",
    "                \n",
    "                header.add_history(\"Bias Subtracted done by quickreduce on {}\".format(date))\n",
    "                \n",
    "                outhdu = fits.PrimaryHDU(data=filnumarray ,header=header)  \n",
    "                \n",
    "                save_hdu(outhdu, save_info, camera, imtype, mask_name,filenum)\n",
    "\n",
    "                ## Plot the image\n",
    "                if make_debug_plots:\n",
    "                    plt.figure()\n",
    "                    plot_array = outhdu.data - np.min(outhdu.data) + 1e-4\n",
    "                    plt.imshow(np.log(plot_array),'gray',origin='lowerleft')\n",
    "                    plt.savefig(filename.replace('.fits','.png'),dpi=1200)\n",
    "                    plt.show()\n",
    "\n",
    "                debiased_hdus[imtype][camera][filenum] = outhdu\n",
    "            print(\"Completed bias subtraction for {}\".format(imtype))\n",
    "            print(\"Results saved to {}\".format(save_info['datadir']))\n",
    "    \n",
    "    dict_of_hdus = debiased_hdus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_info['tags'] = '.b'\n",
    "save_info['tags'] = '.bc' \n",
    "load_info['datadir'] = directory['product']\n",
    "save_info['datadir'] = directory['product']\n",
    "save_info['template'] = filename_template['stitched']\n",
    "load_info['template'] = filename_template['stitched']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if do_step['remove_crs']:\n",
    "    import PyCosmic \n",
    "    for imtype in filenumbers.keys():\n",
    "        for camera in common_info['cameras']:\n",
    "            for filenum in filenumbers[imtype]:\n",
    "                filename = load_info['template'].format(cam=camera, imtype=imtype, maskname=mask_name,\n",
    "                                                             filenum=filenum, tags=load_info['tags'])\n",
    "                filename = os.path.join(load_info['datadir'],filename)\n",
    "                savefile = filename.replace(load_info['tags']+'.fits',save_info['tags']+'.fits')\n",
    "                maskfile = filename.replace(load_info['tags']+'.fits',save_info['tags']+'.crmask.fits')\n",
    "                print(\"\\nFor image type: {}, shoe: {},   filenum: {}\".format(imtype,camera,filenum))\n",
    "                outdat,pycosmask,pyheader = PyCosmic.detCos(filename,maskfile,savefile,rdnoise='ENOISE',sigma_det=8,\n",
    "                                                            gain='EGAIN',verbose=True,return_data=True)\n",
    "                if make_debug_plots:\n",
    "                    plot_cr_images(pycosmask,outdat,maskfile,filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_info['tags'] = '.bc'\n",
    "save_info['tags'] = '.bc'  \n",
    "load_info['datadir'] = directory['product']\n",
    "save_info['datadir'] = directory['oned']\n",
    "load_info['template'] = filename_template['stitched']\n",
    "save_info['template'] = filename_template['oned']\n",
    "\n",
    "if load_data_from_disk_each_step or ('apcut' == start):\n",
    "        dict_of_hdus = get_all_filedata(filenum_dict=filenumbers, \n",
    "                                        **setup_info,**load_info,**master_info)\n",
    "        print_data_neatly(dict_of_hdus)\n",
    "        blank_dictofdicts = get_dict_temp(dict_of_hdus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if do_step['apcut']:\n",
    "    from app_detection_helper_funcs import find_aperatures,cutout_1d_aperatures\n",
    "    from app_detection_helper_funcs import cutout_1d_aperatures\n",
    "\n",
    "    apcut_hdus = blank_dictofdicts.copy()\n",
    "    aperatures = {}\n",
    "    for camera in setup_info['cameras']:\n",
    "        fib_hdus = dict_of_hdus['fibmap'][camera]\n",
    "        first_hdu = list(fib_hdus.values())[0]\n",
    "        sumd_fib_hdu = np.zeros(shape=first_hdu.data.shape)\n",
    "        \n",
    "        for val in fib_hdus.values():\n",
    "            sumd_fib_hdu += val.data\n",
    "        aperature = find_aperatures(sumd_fib_hdu,camera=camera,function_order=4,                                              deadfibers=setup_info['deadfibers'],                                                      resol_factor=int(100),nvertslices=int(2**6)      )  \n",
    "        aperatures[camera] = aperature\n",
    "        for imtype,camdict in dict_of_hdus.items():\n",
    "            filenumdict = camdict[camera]\n",
    "            for filenum,hdu in filenumdict.items():\n",
    "                oneds = cutout_1d_aperatures(hdu.data,aperature)\n",
    "                outhead = hdu.header.copy(strip=True)\n",
    "                outhead.remove('datasec',ignore_missing=True)\n",
    "                outhead.remove('trimsec',ignore_missing=True)\n",
    "                outhead.remove('CHOFFX',ignore_missing=True)\n",
    "                outhead.remove('CHOFFY',ignore_missing=True)\n",
    "                outhead.remove('NOPAMPS',ignore_missing=True)\n",
    "                 \n",
    "                outhdu = fits.BinTableHDU(data=Table(data=oneds),header=outhead)\n",
    "                save_hdu(outhdu, save_info, camera, imtype, mask_name,filenum)\n",
    "\n",
    "                apcut_hdus[imtype][camera][filenum] = outhdu\n",
    "                \n",
    "                if make_debug_plots:\n",
    "                    plt.figure()\n",
    "                    for dat in oneds:\n",
    "                        plt.plot(range(len(dat)),dat)\n",
    "                    plt.show()\n",
    "    dict_of_hdus = apcut_hdus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "filenumbers.pop('fibmap')\n",
    "#master_info['master_types'].append('fibmap')\n",
    "\n",
    "load_info['tags'] = '.bc'\n",
    "save_info['tags'] = '.bc'  \n",
    "load_info['datadir'] = directory['oned']\n",
    "save_info['datadir'] = directory['oned']\n",
    "load_info['template'] = filename_template['oned']\n",
    "save_info['template'] = filename_template['oned']\n",
    "\n",
    "if load_data_from_disk_each_step or ('wavecalib' == start):\n",
    "        dict_of_hdus = get_all_filedata(filenum_dict=filenumbers,fibersplit=True,\n",
    "                                        **setup_info,**load_info,**master_info)\n",
    "        print_data_neatly(dict_of_hdus)\n",
    "        blank_dictofdicts = get_dict_temp(dict_of_hdus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##loop through and rename, no more camera\n",
    "\n",
    "\n",
    "# dict_of_hdus['thar']['r'][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "deltat = np.datetime64('now','m').astype(int)-np.datetime64('2018-06-01T00:00','m').astype(int)\n",
    "print(deltat)\n",
    "#filename = os.path.join(path_to_calibs,'calib_wave_coefs_{}_{}.dat'.format(camera, deltat))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# head1 = dict_of_hdus['thar']['r'][627].header.copy(strip=True)\n",
    "# head2 = dict_of_hdus['thar']['r'][635].header.copy(strip=True)\n",
    "# for key in head1:\n",
    "#     if key == 'COMMENT' or key == 'HISTORY':\n",
    "#         continue\n",
    "#     replace = 'r_'\n",
    "#     if 'FIBER' in key:\n",
    "#         replace += key.replace('FIBER','FIB')\n",
    "#     elif len(key)>6:\n",
    "#         if key[-1].isdigit():\n",
    "#             if key[-2].isdigit():\n",
    "#                 replace+=key[2:]\n",
    "#             else:\n",
    "#                 replace+=key[1:-1]\n",
    "#         else:\n",
    "#             replace+=key[:6]\n",
    "#     else:\n",
    "#         replace+=key\n",
    "#     head1.rename_keyword(key,replace)\n",
    "# for key in head2:\n",
    "#     if key == 'COMMENT' or key == 'HISTORY':\n",
    "#         continue\n",
    "#     replace = 'b_'\n",
    "#     if 'FIBER' in key:\n",
    "#         replace += key.replace('FIBER','FIB')\n",
    "#     elif len(key)>6:\n",
    "#         if key[-1].isdigit():\n",
    "#             if key[-2].isdigit():\n",
    "#                 replace+=key[2:]\n",
    "#             else:\n",
    "#                 replace+=key[1:-1]\n",
    "#         else:\n",
    "#             replace+=key[:6]\n",
    "#     else:\n",
    "#         replace+=key\n",
    "#     head2.rename_keyword(key,replace)\n",
    "# head1.extend(head2)\n",
    "# head1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#dict_of_hdus['thar']['r'][627].header.copy(strip=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# pseudocode\n",
    "\n",
    "# take all comps\n",
    "\n",
    "# loop through each aperature, fit to first comp\n",
    "#                             fit to first thar\n",
    "#                             loop through remainder of filenums (auto after first?)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if do_step['wavecalib']:\n",
    "    load_fromfile_if_possible = False\n",
    "    import re\n",
    "    timestamp = np.datetime64('now', 'm').astype(int) - np.datetime64('2018-06-01T00:00', 'm').astype(int)\n",
    "\n",
    "    from wavelength_calibration_funcs import calibrate_pixels2wavelengths\n",
    "    from calibrations import wavelength_fitting, interactive_wavelength_fitting\n",
    "    from calibrations import load_calibration_lines_salt_dict as load_calibration\n",
    "    from calibrations import save_calib_dict,locate_calib_dict\n",
    "    #from calibrations import load_calibration_lines_nist_dict as load_calibration\n",
    "\n",
    "    complinelistdict = load_calibration(cal_lamp, wavemincut=4500, wavemaxcut=6600)\n",
    "    tharlinelistdict = load_calibration(thar_lamp, wavemincut=4500, wavemaxcut=6600)\n",
    "\n",
    "    calib_coefs = {}\n",
    "    calib_coefs['comp'] = {key:{} for key in dict_of_hdus['comp'][setup_info['cameras'][0]].keys()}\n",
    "    calib_coefs['thar'] = {key:{} for key in dict_of_hdus['thar'][setup_info['cameras'][0]].keys()}\n",
    "    calib_coefs['interactive'] = {key:{} for key in setup_info['cameras']}\n",
    "    \n",
    "    for camera in setup_info['cameras']:\n",
    "        comp_filenums = list(dict_of_hdus['comp'][camera].keys())\n",
    "        thar_filenums = list(dict_of_hdus['thar'][camera].keys())\n",
    "\n",
    "        ## Interactive\n",
    "        fil = comp_filenums[0]\n",
    "        if load_fromfile_if_possible:\n",
    "            coef_table = locate_calib_dict('./', 'interactive',camera,config,fil)\n",
    "            if coef_table is None:\n",
    "                load_fromfile_if_possible = False\n",
    "            else:\n",
    "                calib_coef_table = coef_table\n",
    "\n",
    "        first_comp = (dict_of_hdus['comp'][camera][fil]).data\n",
    "\n",
    "        if not load_fromfile_if_possible:\n",
    "            calib_coef_table = interactive_wavelength_fitting(first_comp,complinelistdict, \\\n",
    "                                                              default = (4523.4,1.0007,-1.6e-6))\n",
    "            calib_coefs['interactive'][camera] = calib_coef_table\n",
    "            save_calib_dict(calib_coef_table,'interactive',camera,config,fil,timestamp)\n",
    "\n",
    "        calib_coefs['interactive'][camera] = calib_coef_table\n",
    "\n",
    "        ## First pointed fit\n",
    "        if load_fromfile_if_possible:\n",
    "            coef_table = locate_calib_dict('./', 'compfit',camera,config,comp_filenums[0])\n",
    "            if coef_table is None:\n",
    "                load_fromfile_if_possible = False\n",
    "            else:\n",
    "                calib_coef_table = coef_table\n",
    "\n",
    "        if not load_fromfile_if_possible:\n",
    "            calib_coef_table, covs, selected_complinelists = wavelength_fitting(first_comp, complinelistdict,\\\n",
    "                                                                          calib_coef_table,select_lines = True)\n",
    "            save_calib_dict(calib_coef_table, 'compfit', camera, config, comp_filenums[0], timestamp)\n",
    "\n",
    "        calib_coefs['comp'][comp_filenums[0]][camera] = calib_coef_table\n",
    "\n",
    "        ## Loop through pointed fits\n",
    "        # for filenum in comp_filenums[1:]:\n",
    "        #     if load_fromfile_if_possible:\n",
    "        #         coef_table = locate_calib_dict('./', 'compfit', camera, config, filenum)\n",
    "        #         if coef_table is None:\n",
    "        #             load_fromfile_if_possible = False\n",
    "        #         else:\n",
    "        #             calib_coef_table = coef_table\n",
    "        #\n",
    "        #     if not load_fromfile_if_possible:\n",
    "        #         comp = dict_of_hdus['comp'][camera][filenum].data\n",
    "        #         calib_coef_table, covs = wavelength_fitting(comp, selected_complinelists, calib_coef_table)\n",
    "        #         save_calib_dict(calib_coef_table, 'compfit', camera, config, filenum, timestamp)\n",
    "        #\n",
    "        #     calib_coefs['comp'][filenum][camera] = calib_coef_table\n",
    "\n",
    "        if load_fromfile_if_possible:\n",
    "            coef_table = locate_calib_dict('./', 'tharfit', camera, config, thar_filenums[0])\n",
    "            if coef_table is None:\n",
    "                load_fromfile_if_possible = False\n",
    "            else:\n",
    "                calib_coef_table = coef_table\n",
    "\n",
    "        if not load_fromfile_if_possible:\n",
    "            first_thar = (dict_of_hdus['thar'][camera][thar_filenums[0]]).data\n",
    "            calib_coef_table, covs, selected_tharlinelists = wavelength_fitting(first_thar, tharlinelistdict, \\\n",
    "                                                                        calib_coef_table,select_lines = True)\n",
    "            save_calib_dict(calib_coef_table, 'tharfit', thar_filenums[0], camera, config, timestamp)\n",
    "\n",
    "        calib_coefs['thar'][thar_filenums[0]][camera] = calib_coef_table\n",
    "\n",
    "        for filenum in thar_filenums[1:]:\n",
    "            if load_fromfile_if_possible:\n",
    "                coef_table = locate_calib_dict('./', 'tharfit', camera, config, filenum)\n",
    "                if coef_table is None:\n",
    "                    load_fromfile_if_possible = False\n",
    "                else:\n",
    "                    calib_coef_table = coef_table\n",
    "\n",
    "            if not load_fromfile_if_possible:\n",
    "                thar = dict_of_hdus['thar'][camera][filenum].data\n",
    "                calib_coef_table, covs = wavelength_fitting(thar,selected_tharlinelists, calib_coef_table)\n",
    "                save_calib_dict(calib_coef_table, 'tharfit', camera, config, filenum, timestamp)\n",
    "\n",
    "            calib_coefs['thar'][filenum][camera] = calib_coef_table\n",
    "\n",
    "    with open('calib_coefs.pkl','wb') as pklout:\n",
    "        pkl.dump(calib_coefs,pklout)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#tab = Table(first_comp)\n",
    "#np.asarray(tab.columns[tab.colnames])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dict_of_hdus = get_all_filedata(filenum_dict=filenumbers,fibersplit=True,\n",
    "                                **setup_info,**load_info,**master_info)\n",
    "print_data_neatly(dict_of_hdus)\n",
    "blank_dictofdicts = get_dict_temp(dict_of_hdus)\n",
    "with open('calib_coefs.pkl','rb') as pklin:\n",
    "    calib_coefs = pkl.load(pklin)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#best_of = calib_coefs['thar'][627]['r']\n",
    "#best_of = calib_coefs['comp'][628]['r']\n",
    "#best_of\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import seaborn\n",
    "def fifthorder(xs,a,b,c,d,e,f):\n",
    "    return a+b*xs+c*xs*xs+d*xs**3+e*xs**4+f*xs**5\n",
    "\n",
    "from quickreduce_funcs import format_plot\n",
    "\n",
    "from calibrations import load_calibration_lines_salt_dict as load_calibration\n",
    "from calibrations import air_to_vacuum\n",
    "def get_linelist(lamp_list):\n",
    "    linelistdict, cal_states = load_calibration(lamp_list, wavemincut=4500, wavemaxcut=6600)\n",
    "    wm, fm = [], []\n",
    "    if 'Xe' in linelistdict.keys():\n",
    "        pass\n",
    "        # wm_Xe, fm_Xe = linelistdict['Xe']\n",
    "        # wm_Xe = air_to_vacuum(wm_Xe)\n",
    "        # wm.extend(wm_Xe)\n",
    "        # fm.extend(fm_Xe)\n",
    "    if 'Ar' in linelistdict.keys():\n",
    "        pass\n",
    "        # wm_Ar, fm_Ar = linelistdict['Ar']\n",
    "        # wm_Ar = air_to_vacuum(wm_Ar)\n",
    "        # wm.extend(wm_Ar)\n",
    "        # fm.extend(fm_Ar)\n",
    "    if 'HgAr' in linelistdict.keys():\n",
    "        wm_HgNe, fm_HgNe = linelistdict['HgAr']\n",
    "        wm_HgNe = air_to_vacuum(wm_HgNe)\n",
    "        wm.extend(wm_HgNe)\n",
    "        fm.extend(fm_HgNe)\n",
    "    if 'NeAr' in linelistdict.keys():\n",
    "        wm_Ne, fm_Ne = linelistdict['NeAr']\n",
    "        wm_Ne = air_to_vacuum(wm_Ne)\n",
    "        wm.extend(wm_Ne)\n",
    "        fm.extend(fm_Ne)\n",
    "    if 'ThAr' in linelistdict.keys():\n",
    "        wm_thar, fm_thar = linelistdict['ThAr']\n",
    "        wm_thar = air_to_vacuum(wm_thar)\n",
    "        wm_thar = np.asarray(wm_thar)\n",
    "        fm_thar = np.asarray(fm_thar)\n",
    "        sorted = np.argsort(fm_thar)[::-1]\n",
    "        wm_thar_sort = wm_thar[sorted]\n",
    "        fm_thar_sort = fm_thar[sorted]\n",
    "        wm_thar_sort = wm_thar_sort[:len(wm_thar_sort)//4]\n",
    "        fm_thar_sort = fm_thar_sort[:len(fm_thar_sort) // 4]\n",
    "        wm.extend(wm_thar_sort.tolist())\n",
    "        fm.extend(fm_thar_sort.tolist())\n",
    "\n",
    "    wm,fm = np.asarray(wm),np.asarray(fm)\n",
    "    ordered = np.argsort(wm)\n",
    "    wm = wm[ordered]\n",
    "    fm = fm[ordered]\n",
    "    return wm,fm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wm,fm = {},{}\n",
    "wm['thar'],fm['thar'] = get_linelist(thar_lamp)\n",
    "wm['comp'],fm['comp'] = get_linelist(cal_lamp)\n",
    "fm['comp'].min(),fm['thar'].min()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fnum = {}\n",
    "fnum['comp'] = 628\n",
    "fnum['thar'] = 627\n",
    "cam = 'r'\n",
    "dict_of_hdus['comp'][cam][fnum['comp']].data['r101']\n",
    "\n",
    "best_fits = {}\n",
    "best_fits['comp'] = calib_coefs['comp'][fnum['comp']][cam]\n",
    "best_fits['thar'] = calib_coefs['thar'][fnum['thar']][cam]\n",
    "\n",
    "best_fits['comp']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for ap in ['r101','r102']:\n",
    "    for calib_cof in ['comp','thar']:\n",
    "        for calib_spec in ['comp','thar']:\n",
    "            #calib_spec = calib_cof\n",
    "            plt.figure()\n",
    "            fig, ax = plt.subplots(1,figsize=(12,8))\n",
    "\n",
    "            for w in wm[calib_spec]:\n",
    "                ax.axvline(w, color='r', alpha=0.5)\n",
    "            line, = ax.plot(wm[calib_spec], fm[calib_spec]/(4*fm[calib_spec].max()), 'ro')\n",
    "            flux = dict_of_hdus[calib_spec][cam][fnum[calib_spec]].data[ap]\n",
    "            pixels = np.arange(flux.size).astype(np.float64)\n",
    "            best_wave = fifthorder(pixels,*best_fits[calib_cof][ap])\n",
    "            fline, = plt.plot(best_wave, flux/flux.max(), 'b')\n",
    "            format_plot(ax,title='Pixel to Vacuum Wavelegth fit coefs:{}, spec:{}, {}'.format(calib_cof,                                                                            calib_spec,ap),                       xlabel=r'${\\lambda}_{vac}\\quad\\mathrm{[\\AA]}$',                       ylabel='Normalized Intensity')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aperature_number_pixoffset('r102')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def aperature_number_pixoffset(fibnum):\n",
    "    if type(fibnum) is str:\n",
    "        strpd_fibnum = fibnum.strip('rb')\n",
    "        if strpd_fibnum.isnumeric():\n",
    "            tet = np.float64(strpd_fibnum[0])-1.\n",
    "            fib = np.float64(strpd_fibnum[1:])-1.\n",
    "        else:\n",
    "            return 0.\n",
    "    elif np.isscalar(fibnum):\n",
    "        tet = fibnum//16\n",
    "        fib = fibnum%16\n",
    "        \n",
    "    c1 =   1.023\n",
    "    c2 =   54.058\n",
    "    c3 =  -6.962\n",
    "    c4 =   1.985\n",
    "    c5 =  -0.5560\n",
    "    \n",
    "    return c1 + c2*tet + c3*tet*tet + c4*fib + c5*tet*fib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from quickreduce_funcs import pair_exposures\n",
    "pair_exposures(dict_of_hdus,cams_same=True,max_matches=1)['r'][631]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "master_info['master_types'].append('twiflat')\n",
    "#master_info['master_types'].append('fibmap')\n",
    "filenumbers.pop('twiflat')\n",
    "filenumbers.pop('fibmap')\n",
    "load_info['tags'] = '.bc'\n",
    "save_info['tags'] = '.bcf'\n",
    "load_info['datadir'] = directory['product']\n",
    "save_info['datadir'] = directory['product']\n",
    "\n",
    "if load_data_from_disk_each_step or ('flat' == start):\n",
    "        dict_of_hdus = get_all_filedata(filenum_dict=filenumbers, \n",
    "                                        **setup_info,**load_info,**master_info)\n",
    "        print_data_neatly(dict_of_hdus)  \n",
    "        blank_dictofdicts = get_dict_temp(dict_of_hdus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if do_step['flat']:\n",
    "    for imtype in ['twiflat','fibmap']:\n",
    "        for camera in cr_removed_data[imtype].keys():\n",
    "            filenum_3d_array = np.asarray(list(data[imtype][camera].values()))\n",
    "            header =list(headers[imtype][camera].values())[0]\n",
    "            \n",
    "            for exposure in range(filenum_3d_array.shape[0]):\n",
    "                ## get exposure and make sure there are no negative values\n",
    "                current_exposure = filenum_3d_array[exposure,:,:]\n",
    "                current_exposure -= np.min(current_exposure)\n",
    "                ## get a median smoothed version to remove any peculiarities, find it's max\n",
    "                median_exposure = median_filter(current_exposure,size=5)\n",
    "                ## divide by the max of the median exposure to normalize (excluding outliers)\n",
    "                current_exposure /= np.max(median_exposure)\n",
    "                filenum_3d_array[exposure,:,:] = current_exposure\n",
    "            \n",
    "            filenum_summed_array = np.median(filenum_3d_array,axis=0)\n",
    "            header.add_history(\"Summed Master {} done by quickreduce on {}\".format(imtype,date))\n",
    "\n",
    "            outhdu = fits.PrimaryHDU(data=filenum_summed_array ,header=header)\n",
    "            outname = master_info['master_templates'].format(cam=camera, imtype=imtype,maskname=mask_name, \n",
    "                                                           tags=save_info['tags'])\n",
    "            filename = os.path.join(save_info['datadir'], outname)\n",
    "            outhdu.writeto( filename ,overwrite=True)\n",
    "\n",
    "            ## Plot the image\n",
    "            plt.figure()\n",
    "            plot_array = outhdu.data - np.min(outhdu.data) + 1e-4\n",
    "            plt.imshow(np.log(plot_array),'gray',origin='lowerleft')\n",
    "            plt.savefig(filename.replace('.fits','.png'),dpi=1200)\n",
    "            plt.show()\n",
    "\n",
    "            headers[imtype][camera] = header\n",
    "            data[imtype][camera] = filenum_summed_array\n",
    "            \n",
    "    master_twiflat_data = data['twiflat']\n",
    "    flat_data, flat_headers = {}, {}\n",
    "    for imtype in filenumbers.keys():\n",
    "        flat_data[imtype] = {}\n",
    "        flat_headers[imtype] = {}\n",
    "        for camera,master_twiflat in master_twiflat_data.items():\n",
    "            master_twiflat /= np.max(master_twiflat)\n",
    "            flat_data[imtype][camera] = {}\n",
    "            flat_headers[imtype][camera] = {}\n",
    "            datadict = data[imtype][camera]\n",
    "            \n",
    "            headerdict = headers[imtype][camera]\n",
    "            for filnum,filearray in datadict.items():\n",
    "                filearray = filearray.astype(float)\n",
    "                header = headerdict[filnum]\n",
    "                filearray /= master_twiflat.astype(float)\n",
    "                header.add_history(\"Flat correction done by quickreduce on {}\".format(date))\n",
    "                outhdu = fits.PrimaryHDU(data=filearray ,header=header)\n",
    "                filename = save_info['template'].format(cam=camera, imtype=imtype, \n",
    "                                                             maskname=mask_name, \n",
    "                                                             filenum=filnum, \\\n",
    "                                                             tags=save_info['tags'])\n",
    "                filename = os.path.join(save_info['datadir'], filename)\n",
    "                outhdu.writeto( filename ,overwrite=True)\n",
    "\n",
    "                ## Plot the image\n",
    "                plt.figure()\n",
    "                plot_array = outhdu.data - np.min(outhdu.data) + 1e-4\n",
    "                plt.imshow(np.log(plot_array),'gray',origin='lowerleft')\n",
    "                plt.savefig(filename.replace('.fits','.png'),dpi=1200)\n",
    "                plt.show() \n",
    "                \n",
    "                flatnd_data[imtype][camera][filnum] = header\n",
    "                flatnd_headers[imtype][camera][filnum] = filearray\n",
    "\n",
    "    print(\"Completed flattening for {}\".format(imtype))\n",
    "    print(\"Results saved to {}\".format(save_info['datadir']))\n",
    "    del cr_removed_data, cr_removed_headers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if load_data_from_disk_each_step or ('combine' == start):\n",
    "        dict_of_hdus = get_all_filedata(filenum_dict=filenumbers, \n",
    "                                        **setup_info,**load_info,**master_info)\n",
    "        print_data_neatly(dict_of_hdus)\n",
    "        blank_dictofdicts = get_dict_temp(dict_of_hdus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if load_data_from_disk_each_step or ('zfit' == start):\n",
    "        dict_of_hdus = get_all_filedata(filenum_dict=filenumbers, \n",
    "                                        **setup_info,**load_info,**master_info)\n",
    "        print_data_neatly(dict_of_hdus)\n",
    "        blank_dictofdicts = get_dict_temp(dict_of_hdus)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
